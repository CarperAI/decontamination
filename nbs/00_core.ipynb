{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset, load_from_disk, Features, Sequence, Value\n",
    "from datasketch import LeanMinHash, MinHash, MinHashLSH\n",
    "from rich.logging import RichHandler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "datasets.logging.set_verbosity_error()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(RichHandler(rich_tracebacks=True))\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MINHASH_SEED = 42\n",
    "NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hash_content(\n",
    "    idx: int, # index of the document\n",
    "    content: str, # content of the document\n",
    "    *,\n",
    "    num_perm: int # number of permutations\n",
    "    ): # The MinHash signature and the index of the record.\n",
    "    \"\"\"\n",
    "    Hash the content of a record using MinHash. This function should be\n",
    "    used with multiprocessing and it scales well with the number of cores.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm, seed=MINHASH_SEED)\n",
    "    m.update_batch([token.encode(\"utf-8\") for token in {t for t in NON_ALPHA.split(content) if t}])\n",
    "    return {\"__signature__\": m.hashvalues, \"__id__\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hash_content(0, \"Hello world!\", num_perm=128)\n",
    "assert result[\"__id__\"] == 0\n",
    "assert result[\"__signature__\"].shape == (128,)\n",
    "assert result[\"__signature__\"].dtype == np.dtype('uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def query_content(\n",
    "    idx: int, # index of the document\n",
    "    signature: np.ndarray, # MinHash signature of the document\n",
    "    *,\n",
    "    index: MinHashLSH # The MinHashLSH index. It is shared across all processes when using multiprocessing with fork without copy.\n",
    "    ): # The query result.\n",
    "    \"\"\"\n",
    "    Query the MinHashLSH index for the record. This function can be used with multiprocessing\n",
    "    as long as the index is shared across processes.\n",
    "    Parameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"__neighbors__\": [\n",
    "            str(dup_idx)\n",
    "            for dup_idx in index.query(\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=signature),\n",
    "            )\n",
    "        ],\n",
    "        \"__id__\": idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaccard_similarity(\n",
    "    s1: str, # The first string to compare.\n",
    "    s2: str # The second string to compare.\n",
    "    ) -> float: # The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    Calculate the jaccard similarity between two code snippets.\n",
    "    \"\"\"\n",
    "    tokens1 = set([t for t in NON_ALPHA.split(s1) if t.strip()])\n",
    "    tokens2 = set([t for t in NON_ALPHA.split(s2) if t.strip()])\n",
    "    return len(tokens1 & tokens2) / max(1, len(tokens1 | tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\n",
    "assert jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_list_to_dict(list):\n",
    "    result = {}\n",
    "    for item in list:\n",
    "        config = item['config']\n",
    "        split = item['split']\n",
    "        if split == \"train\": continue\n",
    "        if config in result:\n",
    "            result[config].append(split)\n",
    "        else:\n",
    "            result[config] = [split]\n",
    "    \n",
    "    final_result = {}\n",
    "    for config, splits in result.items():\n",
    "        if \"all\" in config:\n",
    "            final_result = {config: splits}\n",
    "            break\n",
    "        else:\n",
    "            final_result[config] = splits\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_lists(name):\n",
    "    token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    if token is None:\n",
    "        raise ValueError(\"HF_ACCESS_TOKEN is not set\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    API_URL = f\"https://datasets-server.huggingface.co/splits?dataset={name}\"\n",
    "    def query():\n",
    "        response = requests.request(\"GET\", API_URL, headers=headers)\n",
    "        return response.json()\n",
    "    data = query()\n",
    "\n",
    "    return convert_list_to_dict(data[\"splits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plain_text': ['test', 'validation']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = \"<TOKEN>\"\n",
    "ds_dict = config_lists(\"lambada\")\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_ds_config(name, ds_dict):\n",
    "    for config, splits in ds_dict.items():\n",
    "        for split in splits:\n",
    "            # print(name, config, split)\n",
    "            ds = load_dataset(name, config, split=split)\n",
    "            remove_columns = []\n",
    "            for column, val_type in ds.features.items():\n",
    "                if val_type.dtype != \"string\":\n",
    "                    remove_columns.append(column)\n",
    "            \n",
    "            ds = ds.remove_columns(remove_columns)\n",
    "            yield ds, f\"{name}_{config}_{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'domain': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "\n",
    "ds, name = next(process_ds_config(\"lambada\", ds_dict))\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean the benchmark dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        benchmark_names: list, # The list of benchmark names to clean.\n",
    "        output_dir: str, # The output directory to save the cleaned datasets and intermediate results.\n",
    "        threshold: float = 0.5, # The threshold to use for the MinHashLSH index.\n",
    "        num_perm: int = 128 # The number of permutations to use for the MinHashLSH index.\n",
    "        ):\n",
    "        self.bm_names = benchmark_names\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.hash_benchmark_datasets()\n",
    "    \n",
    "    def hash_benchmark_datasets(self):\n",
    "        self.benchmarks_paths = [os.path.join(self.output_dir, d) for d in os.listdir(self.output_dir) if os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        if len(self.benchmarks_paths) == 0:\n",
    "            for name in self.bm_names:\n",
    "                ds_dict = config_lists(name)\n",
    "                for benchmark_ds, config_name in process_ds_config(name, ds_dict):\n",
    "                    benchmark_ds = benchmark_ds.map(\n",
    "                            function=lambda x, idx: {\n",
    "                                **hash_content(\n",
    "                                    idx,\n",
    "                                    \" \".join(\n",
    "                                        [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                    ),\n",
    "                                    num_perm=self.num_perm,\n",
    "                                ),\n",
    "                                \"__content__\": \" \".join(\n",
    "                                    [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                ),\n",
    "                            },\n",
    "                            num_proc=4,\n",
    "                            with_indices=True,\n",
    "                            desc=f\"Fingerprinting...\",\n",
    "                        )\n",
    "                    # Save the benchmark dataset.\n",
    "                    benchmarks_path = os.path.join(self.output_dir, config_name)\n",
    "                    benchmark_ds.save_to_disk(benchmarks_path, max_shard_size=\"1GB\")\n",
    "                    self.benchmarks_paths.append(benchmarks_path)\n",
    "        else:\n",
    "            logger.info(\"Benchmark datasets already exist. Skipping hashing.\")\n",
    "\n",
    "    def clean(\n",
    "        self,\n",
    "        ds: Dataset, # The dataset to clean.\n",
    "        column: str, # The column to clean.\n",
    "        check_for_fp: bool = True, # Whether to check for false positives.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Clean the dataset. This function does the following:\n",
    "        1. Hash the content of the provided dataset using MinHash.\n",
    "        2. Iterate over the benchmark datasets and hash their content.\n",
    "        3. Query the MinHashLSH index for each record in the provided dataset against the benchmark datasets.\n",
    "        4. Filter out the records that have a high similarity with the benchmark datasets.\n",
    "        5. Return the cleaned dataset.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        DATA_SIZE = len(ds)\n",
    "        ds = ds.map(\n",
    "            lambda _, idx: {\"__id__\": idx},\n",
    "            with_indices=True,\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Adding index...\",\n",
    "        )\n",
    "        hashed_ds = ds.map(\n",
    "            function=hash_content,\n",
    "            fn_kwargs={\"num_perm\": self.num_perm},\n",
    "            input_columns=[\"__id__\", column],\n",
    "            remove_columns=[column],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=f\"Fingerprinting...\",\n",
    "        )\n",
    "        # remove unused columns\n",
    "        hashed_ds = hashed_ds.remove_columns([c for c in hashed_ds.column_names if c not in [\"__id__\", \"__signature__\"]])\n",
    "        dup_ids = set() # The set of duplicate ids that should be filtered out.\n",
    "        # Iterate over the benchmark datasets, hash their content and query the MinHashLSH index.\n",
    "        for path in self.benchmarks_paths:\n",
    "            globals()[path] = MinHashLSH(\n",
    "                threshold=self.threshold,\n",
    "                num_perm=self.num_perm,\n",
    "            )\n",
    "            hashed_benchmark_ds = load_from_disk(path)\n",
    "            # Update the global variable with the MinHashLSH index.\n",
    "            with globals()[path].insertion_session() as session:\n",
    "                for record in hashed_benchmark_ds:\n",
    "                    session.insert(record[\"__id__\"], LeanMinHash(seed=MINHASH_SEED, hashvalues=record[\"__signature__\"]))\n",
    "            \n",
    "            queried = hashed_ds.map(\n",
    "                function=lambda x, y: query_content(x, y, index=globals()[path]),\n",
    "                num_proc=os.cpu_count(),\n",
    "                input_columns=[\n",
    "                    \"__id__\",\n",
    "                    \"__signature__\",\n",
    "                ],\n",
    "                remove_columns=[\"__signature__\"],\n",
    "                desc=\"Querying...\",\n",
    "                features=Features(\n",
    "                    {\n",
    "                        \"__id__\": Value(\"uint64\"),\n",
    "                        \"__neighbors__\": Sequence(Value(\"string\")),\n",
    "                    }\n",
    "                ),\n",
    "            ).filter(\n",
    "                lambda x: len(x[\"__neighbors__\"]) > 0,\n",
    "                num_proc=os.cpu_count(),\n",
    "                desc=f\"Filtering...\",\n",
    "            )\n",
    "\n",
    "            # Update the set of duplicate ids.\n",
    "            for record in tqdm(\n",
    "                queried,\n",
    "                desc=f\"Checking for false positives...\" if check_for_fp else f\"Filtering...\",\n",
    "            ):\n",
    "                if check_for_fp:\n",
    "                    neighbors = set(record[\"__neighbors__\"])\n",
    "                    curr_text = ds[record[\"__id__\"]][column]\n",
    "                    for neighbor in neighbors:\n",
    "                        reference = hashed_benchmark_ds[int(neighbor)]\n",
    "                        reference_text = reference[\"__content__\"]\n",
    "                        if jaccard_similarity(curr_text, reference_text) >= self.threshold:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                dup_ids.add(record[\"__id__\"])\n",
    "\n",
    "        # Filter out the duplicate ids.\n",
    "        final_data = ds.filter(\n",
    "            lambda idx: idx not in dup_ids,\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Filtering duplicates...\",\n",
    "        )\n",
    "\n",
    "        FINAL_DATA_SIZE = len(final_data)\n",
    "        DUP_SIZE = DATA_SIZE - FINAL_DATA_SIZE\n",
    "\n",
    "        logger.info(f\"{'Data Number':<30}: {DATA_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Number':<30}: {DUP_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Rate':<30}: {DUP_SIZE / DATA_SIZE:.2%}\")\n",
    "        logger.info(f\"{'Total Time':<30}: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/24/23 00:36:00] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Benchmark datasets already exist. Skipping hashing.                   <a href=\"file:///tmp/ipykernel_327294/3257198038.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3257198038.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_327294/3257198038.py#49\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/24/23 00:36:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Benchmark datasets already exist. Skipping hashing.                   \u001b]8;id=67037;file:///tmp/ipykernel_327294/3257198038.py\u001b\\\u001b[2m3257198038.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=479560;file:///tmp/ipykernel_327294/3257198038.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'BenchmarkCleaner.clean.<locals>.check_fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mbigcode/the-stack-smol\u001b[39m\u001b[39m\"\u001b[39m, data_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/python\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m bench_cleaner \u001b[39m=\u001b[39m BenchmarkCleaner(benchmark_names, \u001b[39m\"\u001b[39m\u001b[39m/tmp/decontamination\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, num_perm\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ds \u001b[39m=\u001b[39m bench_cleaner\u001b[39m.\u001b[39;49mclean(ds, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[42], line 146\u001b[0m, in \u001b[0;36mBenchmarkCleaner.clean\u001b[0;34m(self, ds, column, check_for_fp)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# make the above code multi-threaded\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pool \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mPool(\u001b[39m4\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m ids \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(check_fp, queried)\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m ids:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/decontamination/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/decontamination/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/decontamination/lib/python3.10/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     put(task)\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/decontamination/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/miniconda3/envs/decontamination/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'BenchmarkCleaner.clean.<locals>.check_fp'"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "benchmark_names = [\"openai_humaneval\", \"mbpp\"]\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\n",
    "bench_cleaner = BenchmarkCleaner(benchmark_names, \"/tmp/decontamination\", threshold=0.1, num_perm=128)\n",
    "ds = bench_cleaner.clean(ds, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decontamination",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
