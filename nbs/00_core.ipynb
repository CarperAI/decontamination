{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import concatenate_datasets, Dataset, load_dataset, load_from_disk, Features, Sequence, Value\n",
    "from datasketch import LeanMinHash, MinHash, MinHashLSH\n",
    "from pathlib import Path\n",
    "from rich.logging import RichHandler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "datasets.logging.set_verbosity_error()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(RichHandler(rich_tracebacks=True))\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MINHASH_SEED = 42\n",
    "NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hash_content(\n",
    "    idx: int, # index of the document\n",
    "    content: str, # content of the document\n",
    "    *,\n",
    "    num_perm: int # number of permutations\n",
    "    ): # The MinHash signature and the index of the record.\n",
    "    \"\"\"\n",
    "    Hash the content of a record using MinHash. This function should be\n",
    "    used with multiprocessing and it scales well with the number of cores.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm, seed=MINHASH_SEED)\n",
    "    m.update_batch([token.encode(\"utf-8\") for token in {t for t in NON_ALPHA.split(content) if t}])\n",
    "    return {\"__signature__\": m.hashvalues, \"__id__\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hash_content(0, \"Hello world!\", num_perm=128)\n",
    "assert result[\"__id__\"] == 0\n",
    "assert result[\"__signature__\"].shape == (128,)\n",
    "assert result[\"__signature__\"].dtype == np.dtype('uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def query_content(\n",
    "    idx: int, # index of the document\n",
    "    signature: np.ndarray, # MinHash signature of the document\n",
    "    *,\n",
    "    index: MinHashLSH # The MinHashLSH index. It is shared across all processes when using multiprocessing with fork without copy.\n",
    "    ): # The query result.\n",
    "    \"\"\"\n",
    "    Query the MinHashLSH index for the record. This function can be used with multiprocessing\n",
    "    as long as the index is shared across processes.\n",
    "    Parameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"__neighbors__\": [\n",
    "            str(dup_idx)\n",
    "            for dup_idx in index.query(\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=signature),\n",
    "            )\n",
    "        ],\n",
    "        \"__id__\": idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaccard_similarity(\n",
    "    s1: str, # The first string to compare.\n",
    "    s2: str # The second string to compare.\n",
    "    ) -> float: # The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    Calculate the jaccard similarity between two code snippets.\n",
    "    \"\"\"\n",
    "    tokens1 = set([t for t in NON_ALPHA.split(s1) if t.strip()])\n",
    "    tokens2 = set([t for t in NON_ALPHA.split(s2) if t.strip()])\n",
    "    return len(tokens1 & tokens2) / max(1, len(tokens1 | tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\n",
    "assert jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_list_to_dict(list):\n",
    "    result = {}\n",
    "    for item in list:\n",
    "        config = item['config']\n",
    "        split = item['split']\n",
    "        if split == \"train\": continue\n",
    "        if config in result:\n",
    "            result[config].append(split)\n",
    "        else:\n",
    "            result[config] = [split]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_lists(name):\n",
    "    token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    if token is None:\n",
    "        raise ValueError(\"HF_ACCESS_TOKEN is not set\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    API_URL = f\"https://datasets-server.huggingface.co/splits?dataset={name}\"\n",
    "    def query():\n",
    "        response = requests.request(\"GET\", API_URL, headers=headers)\n",
    "        return response.json()\n",
    "    data = query()\n",
    "\n",
    "    return convert_list_to_dict(data[\"splits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plain_text': ['test', 'validation']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = \"<TOKEN>\"\n",
    "ds_dict = config_lists(\"lambada\")\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_ds_config(name, ds_dict, output_dir):\n",
    "    for config, splits in ds_dict.items():\n",
    "        for split in splits:\n",
    "            config_name = f\"{name}_{config}_{split}\"\n",
    "            benchmarks_path = os.path.join(output_dir, config_name)\n",
    "            try:\n",
    "                ds = load_dataset(name, config, split=split, num_proc=os.cpu_count())\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "                logger.error(f\"Failed to load {name} {config} {split}\")\n",
    "                continue\n",
    "            remove_columns = []\n",
    "            for column, val_type in ds.features.items():\n",
    "                if val_type.dtype != \"string\":\n",
    "                    remove_columns.append(column)\n",
    "            \n",
    "            ds = ds.remove_columns(remove_columns)\n",
    "            yield ds, config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'domain': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ds, name = next(process_ds_config(\"lambada\", ds_dict, \"/tmp\"))\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_record(record, check_for_fp, ds, column, benchmarks, threshold):\n",
    "    if check_for_fp:\n",
    "        neighbors = set(record[\"__neighbors__\"])\n",
    "        curr_text = ds[record[\"__id__\"]][column]\n",
    "        for neighbor in neighbors:\n",
    "            reference = benchmarks[int(neighbor)]\n",
    "            reference_text = reference[\"__content__\"]\n",
    "            if jaccard_similarity(curr_text, reference_text) >= threshold:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "    return record[\"__id__\"]\n",
    "\n",
    "def parallelized_function(queried, check_for_fp, ds, column, benchmarks, threshold, num_workers):\n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        results = pool.starmap(process_record, [(record, check_for_fp, ds, column, benchmarks, threshold) for record in queried])\n",
    "        dup_ids = {result for result in results if result is not None}\n",
    "    return dup_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean the benchmark dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        benchmark_names: list, # The list of benchmark names to clean.\n",
    "        output_dir: str, # The output directory to save the cleaned datasets and intermediate results.\n",
    "        threshold: float = 0.5, # The threshold to use for the MinHashLSH index.\n",
    "        num_perm: int = 128, # The number of permutations to use for the MinHashLSH index.\n",
    "        num_workers: int = 1 # The number of workers to use for the MinHashLSH index.\n",
    "    ):\n",
    "        self.bm_names = benchmark_names\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.num_workers = num_workers\n",
    "        self.hash_benchmark_datasets()\n",
    "\n",
    "    def hash_benchmark_datasets(self):\n",
    "        # grab all directories in the output directory and subdirectories\n",
    "        self.benchmarks_paths = [\n",
    "            str(path.parent)\n",
    "            for path in Path(self.output_dir).rglob(\"*.json\")\n",
    "        ]\n",
    "        if len(self.benchmarks_paths) == 0:\n",
    "            global_idx = 0\n",
    "            for name in self.bm_names:\n",
    "                ds_dict = config_lists(name)\n",
    "                for benchmark_ds, config_name in process_ds_config(name, ds_dict, self.output_dir,):\n",
    "                    benchmark_ds = benchmark_ds.map(\n",
    "                            function=lambda x: {\n",
    "                                **hash_content(\n",
    "                                    global_idx,\n",
    "                                    \" \".join(\n",
    "                                        [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                    ),\n",
    "                                    num_perm=self.num_perm,\n",
    "                                ),\n",
    "                                \"__content__\": \" \".join(\n",
    "                                    [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                ),\n",
    "                            },\n",
    "                            num_proc=self.num_workers,\n",
    "                            desc=f\"Fingerprinting...\",\n",
    "                        )\n",
    "                    # Save the benchmark dataset.\n",
    "                    benchmarks_path = os.path.join(self.output_dir, config_name)\n",
    "                    benchmark_ds.save_to_disk(benchmarks_path, max_shard_size=\"1GB\")\n",
    "                    self.benchmarks_paths.append(benchmarks_path)\n",
    "                    global_idx += 1\n",
    "        else:\n",
    "            logger.info(\"Benchmark datasets already exist. Skipping hashing.\")\n",
    "\n",
    "    def clean(\n",
    "        self,\n",
    "        ds: Dataset, # The dataset to clean.\n",
    "        column: str, # The column to clean.\n",
    "        check_for_fp: bool = True, # Whether to check for false positives.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Clean the dataset. This function does the following:\n",
    "        1. Hash the content of the provided dataset using MinHash.\n",
    "        2. Iterate over the benchmark datasets and hash their content.\n",
    "        3. Query the MinHashLSH index for each record in the provided dataset against the benchmark datasets.\n",
    "        4. Filter out the records that have a high similarity with the benchmark datasets.\n",
    "        5. Return the cleaned dataset.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        DATA_SIZE = len(ds)\n",
    "        ds = ds.map(\n",
    "            lambda _, idx: {\"__id__\": idx},\n",
    "            with_indices=True,\n",
    "            num_proc=self.num_workers,\n",
    "            desc=\"Adding index to dataset...\",\n",
    "        )\n",
    "        hashed_ds = ds.map(\n",
    "            function=hash_content,\n",
    "            fn_kwargs={\"num_perm\": self.num_perm},\n",
    "            input_columns=[\"__id__\", column],\n",
    "            remove_columns=[column],\n",
    "            num_proc=self.num_workers,\n",
    "            desc=f\"Fingerprinting dataset...\",\n",
    "        )\n",
    "        # remove unused columns\n",
    "        hashed_ds = hashed_ds.remove_columns([c for c in hashed_ds.column_names if c not in [\"__id__\", \"__signature__\"]])\n",
    "        benchmarks = []\n",
    "        for path in tqdm(self.benchmarks_paths, desc=\"Loading benchmark datasets...\"):\n",
    "            benchmarks.append(load_from_disk(path))\n",
    "        benchmarks = concatenate_datasets(benchmarks)\n",
    "        # Update indices to be global.\n",
    "        ids = [i for i in range(len(benchmarks))]\n",
    "        benchmarks = benchmarks.remove_columns([\"__id__\"])\n",
    "        benchmarks = benchmarks.add_column(\"__id__\", ids)\n",
    "        minhash_path = os.path.join(self.output_dir, \"minhash_index.pkl\")\n",
    "        if os.path.exists(minhash_path):\n",
    "            logger.info(\"MinHashLSH index already exists. Loading from disk...\")\n",
    "            with open(minhash_path, \"rb\") as f:\n",
    "                minhash = pickle.load(f)\n",
    "        else:\n",
    "            logger.info(\"MinHashLSH index does not exist. Creating...\")\n",
    "            # Create the MinHashLSH index.\n",
    "            minhash = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)\n",
    "            with minhash.insertion_session() as session:\n",
    "                for record in tqdm(benchmarks, desc=\"Inserting benchmarks into MinHashLSH index...\"):\n",
    "                    session.insert(record[\"__id__\"], LeanMinHash(seed=MINHASH_SEED, hashvalues=record[\"__signature__\"]))\n",
    "\n",
    "            # Save the MinHashLSH index.\n",
    "            with open(os.path.join(self.output_dir, \"minhash_index.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(minhash, f)\n",
    "            \n",
    "            logger.info(\"MinHashLSH index created and saved to disk.\")\n",
    "        \n",
    "        logger.info(\"Querying MinHashLSH index...\")\n",
    "        # Query the MinHashLSH index for each record in the provided dataset against the benchmark datasets.\n",
    "        queried = hashed_ds.map(\n",
    "            function=lambda x, y: query_content(x, y, index=minhash),\n",
    "            input_columns=[\n",
    "                \"__id__\",\n",
    "                \"__signature__\",\n",
    "            ],\n",
    "            remove_columns=[\"__signature__\"],\n",
    "            desc=\"Querying...\",\n",
    "            features=Features(\n",
    "                {\n",
    "                    \"__id__\": Value(\"uint64\"),\n",
    "                    \"__neighbors__\": Sequence(Value(\"string\")),\n",
    "                }\n",
    "            ),\n",
    "        ).filter(\n",
    "            lambda x: len(x[\"__neighbors__\"]) > 0,\n",
    "            num_proc=self.num_workers,\n",
    "            desc=f\"Filtering...\",\n",
    "        )\n",
    "\n",
    "        dup_ids = parallelized_function(queried, check_for_fp, ds, column,benchmarks, self.threshold, self.num_workers)\n",
    "        \n",
    "        # Filter out the duplicate ids.\n",
    "        final_data = ds.filter(\n",
    "            lambda idx: idx not in dup_ids,\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=self.num_workers,\n",
    "            desc=\"Filtering duplicates...\",\n",
    "        )\n",
    "\n",
    "        FINAL_DATA_SIZE = len(final_data)\n",
    "        DUP_SIZE = DATA_SIZE - FINAL_DATA_SIZE\n",
    "\n",
    "        logger.info(f\"{'Data Number':<30}: {DATA_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Number':<30}: {DUP_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Rate':<30}: {DUP_SIZE / DATA_SIZE:.2%}\")\n",
    "        logger.info(f\"{'Total Time':<30}: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/13/23 04:35:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Benchmark datasets already exist. Skipping hashing.                   <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#56\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/13/23 04:35:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Benchmark datasets already exist. Skipping hashing.                   \u001b]8;id=215809;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=161257;file:///tmp/ipykernel_1101400/4001702524.py#56\u001b\\\u001b[2m56\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading benchmark datasets...:   0%|          | 0/14 [00:00<?, ?it/s]/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n",
      "Loading benchmark datasets...: 100%|██████████| 14/14 [00:00<00:00, 662.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> MinHashLSH index already exists. Loading from disk<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#100\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m MinHashLSH index already exists. Loading from disk\u001b[33m...\u001b[0m                \u001b]8;id=622650;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=743056;file:///tmp/ipykernel_1101400/4001702524.py#100\u001b\\\u001b[2m100\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Querying MinHashLSH index<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                         <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#117\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Querying MinHashLSH index\u001b[33m...\u001b[0m                                         \u001b]8;id=315859;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=711003;file:///tmp/ipykernel_1101400/4001702524.py#117\u001b\\\u001b[2m117\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Data Number                   : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>                                <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#152\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Data Number                   : \u001b[1;36m10000\u001b[0m                                \u001b]8;id=678039;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=216456;file:///tmp/ipykernel_1101400/4001702524.py#152\u001b\\\u001b[2m152\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Duplicate Number              : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                    <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#153\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Duplicate Number              : \u001b[1;36m0\u001b[0m                                    \u001b]8;id=232427;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=489795;file:///tmp/ipykernel_1101400/4001702524.py#153\u001b\\\u001b[2m153\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Duplicate Rate                : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%                                <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#154\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Duplicate Rate                : \u001b[1;36m0.00\u001b[0m%                                \u001b]8;id=322831;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=92737;file:///tmp/ipykernel_1101400/4001702524.py#154\u001b\\\u001b[2m154\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Total Time                    : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.26</span> seconds                         <a href=\"file:///tmp/ipykernel_1101400/4001702524.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4001702524.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1101400/4001702524.py#155\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total Time                    : \u001b[1;36m0.26\u001b[0m seconds                         \u001b]8;id=52812;file:///tmp/ipykernel_1101400/4001702524.py\u001b\\\u001b[2m4001702524.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=796787;file:///tmp/ipykernel_1101400/4001702524.py#155\u001b\\\u001b[2m155\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "benchmark_names = [\"openai_humaneval\", \"mbpp\"]\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\n",
    "bench_cleaner = BenchmarkCleaner(benchmark_names, \"/tmp/decontamination\", threshold=0.85, num_perm=256)\n",
    "ds = bench_cleaner.clean(ds, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decontamination",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
