{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset, load_from_disk, Features, Sequence, Value\n",
    "from datasketch import LeanMinHash, MinHash, MinHashLSH\n",
    "from pathlib import Path\n",
    "from rich.logging import RichHandler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "datasets.logging.set_verbosity_error()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(RichHandler(rich_tracebacks=True))\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MINHASH_SEED = 42\n",
    "NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hash_content(\n",
    "    idx: int, # index of the document\n",
    "    content: str, # content of the document\n",
    "    *,\n",
    "    num_perm: int # number of permutations\n",
    "    ): # The MinHash signature and the index of the record.\n",
    "    \"\"\"\n",
    "    Hash the content of a record using MinHash. This function should be\n",
    "    used with multiprocessing and it scales well with the number of cores.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm, seed=MINHASH_SEED)\n",
    "    m.update_batch([token.encode(\"utf-8\") for token in {t for t in NON_ALPHA.split(content) if t}])\n",
    "    return {\"__signature__\": m.hashvalues, \"__id__\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hash_content(0, \"Hello world!\", num_perm=128)\n",
    "assert result[\"__id__\"] == 0\n",
    "assert result[\"__signature__\"].shape == (128,)\n",
    "assert result[\"__signature__\"].dtype == np.dtype('uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def query_content(\n",
    "    idx: int, # index of the document\n",
    "    signature: np.ndarray, # MinHash signature of the document\n",
    "    *,\n",
    "    index: MinHashLSH # The MinHashLSH index. It is shared across all processes when using multiprocessing with fork without copy.\n",
    "    ): # The query result.\n",
    "    \"\"\"\n",
    "    Query the MinHashLSH index for the record. This function can be used with multiprocessing\n",
    "    as long as the index is shared across processes.\n",
    "    Parameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"__neighbors__\": [\n",
    "            str(dup_idx)\n",
    "            for dup_idx in index.query(\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=signature),\n",
    "            )\n",
    "        ],\n",
    "        \"__id__\": idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaccard_similarity(\n",
    "    s1: str, # The first string to compare.\n",
    "    s2: str # The second string to compare.\n",
    "    ) -> float: # The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    Calculate the jaccard similarity between two code snippets.\n",
    "    \"\"\"\n",
    "    tokens1 = set([t for t in NON_ALPHA.split(s1) if t.strip()])\n",
    "    tokens2 = set([t for t in NON_ALPHA.split(s2) if t.strip()])\n",
    "    return len(tokens1 & tokens2) / max(1, len(tokens1 | tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\n",
    "assert jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_list_to_dict(list):\n",
    "    result = {}\n",
    "    for item in list:\n",
    "        config = item['config']\n",
    "        split = item['split']\n",
    "        if split == \"train\": continue\n",
    "        if config in result:\n",
    "            result[config].append(split)\n",
    "        else:\n",
    "            result[config] = [split]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_lists(name):\n",
    "    token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    if token is None:\n",
    "        raise ValueError(\"HF_ACCESS_TOKEN is not set\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    API_URL = f\"https://datasets-server.huggingface.co/splits?dataset={name}\"\n",
    "    def query():\n",
    "        response = requests.request(\"GET\", API_URL, headers=headers)\n",
    "        return response.json()\n",
    "    data = query()\n",
    "\n",
    "    return convert_list_to_dict(data[\"splits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plain_text': ['test', 'validation']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = \"<TOKEN>\"\n",
    "ds_dict = config_lists(\"lambada\")\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_ds_config(name, ds_dict):\n",
    "    for config, splits in ds_dict.items():\n",
    "        for split in splits:\n",
    "            # print(name, config, split)\n",
    "            ds = load_dataset(name, config, split=split)\n",
    "            remove_columns = []\n",
    "            for column, val_type in ds.features.items():\n",
    "                if val_type.dtype != \"string\":\n",
    "                    remove_columns.append(column)\n",
    "            \n",
    "            ds = ds.remove_columns(remove_columns)\n",
    "            yield ds, f\"{name}_{config}_{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'domain': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ds, name = next(process_ds_config(\"lambada\", ds_dict))\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean the benchmark dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        benchmark_names: list, # The list of benchmark names to clean.\n",
    "        output_dir: str, # The output directory to save the cleaned datasets and intermediate results.\n",
    "        threshold: float = 0.5, # The threshold to use for the MinHashLSH index.\n",
    "        num_perm: int = 128 # The number of permutations to use for the MinHashLSH index.\n",
    "        ):\n",
    "        self.bm_names = benchmark_names\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.hash_benchmark_datasets()\n",
    "    \n",
    "    def hash_benchmark_datasets(self):\n",
    "        # grab all directories in the output directory and subdirectories\n",
    "        self.benchmarks_paths = [\n",
    "            str(path.parent)\n",
    "            for path in Path(self.output_dir).rglob(\"*.json\")\n",
    "        ]\n",
    "        if len(self.benchmarks_paths) == 0:\n",
    "            for name in self.bm_names:\n",
    "                ds_dict = config_lists(name)\n",
    "                for benchmark_ds, config_name in process_ds_config(name, ds_dict):\n",
    "                    benchmark_ds = benchmark_ds.map(\n",
    "                            function=lambda x, idx: {\n",
    "                                **hash_content(\n",
    "                                    idx,\n",
    "                                    \" \".join(\n",
    "                                        [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                    ),\n",
    "                                    num_perm=self.num_perm,\n",
    "                                ),\n",
    "                                \"__content__\": \" \".join(\n",
    "                                    [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                ),\n",
    "                            },\n",
    "                            num_proc=4,\n",
    "                            with_indices=True,\n",
    "                            desc=f\"Fingerprinting...\",\n",
    "                        )\n",
    "                    # Save the benchmark dataset.\n",
    "                    benchmarks_path = os.path.join(self.output_dir, config_name)\n",
    "                    benchmark_ds.save_to_disk(benchmarks_path, max_shard_size=\"1GB\")\n",
    "                    self.benchmarks_paths.append(benchmarks_path)\n",
    "        else:\n",
    "            logger.info(\"Benchmark datasets already exist. Skipping hashing.\")\n",
    "\n",
    "    def clean(\n",
    "        self,\n",
    "        ds: Dataset, # The dataset to clean.\n",
    "        column: str, # The column to clean.\n",
    "        check_for_fp: bool = True, # Whether to check for false positives.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Clean the dataset. This function does the following:\n",
    "        1. Hash the content of the provided dataset using MinHash.\n",
    "        2. Iterate over the benchmark datasets and hash their content.\n",
    "        3. Query the MinHashLSH index for each record in the provided dataset against the benchmark datasets.\n",
    "        4. Filter out the records that have a high similarity with the benchmark datasets.\n",
    "        5. Return the cleaned dataset.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        DATA_SIZE = len(ds)\n",
    "        ds = ds.map(\n",
    "            lambda _, idx: {\"__id__\": idx},\n",
    "            with_indices=True,\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Adding index...\",\n",
    "        )\n",
    "        hashed_ds = ds.map(\n",
    "            function=hash_content,\n",
    "            fn_kwargs={\"num_perm\": self.num_perm},\n",
    "            input_columns=[\"__id__\", column],\n",
    "            remove_columns=[column],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=f\"Fingerprinting...\",\n",
    "        )\n",
    "        # remove unused columns\n",
    "        hashed_ds = hashed_ds.remove_columns([c for c in hashed_ds.column_names if c not in [\"__id__\", \"__signature__\"]])\n",
    "        dup_ids = set() # The set of duplicate ids that should be filtered out.\n",
    "        # Iterate over the benchmark datasets, hash their content and query the MinHashLSH index.\n",
    "        for path in self.benchmarks_paths:\n",
    "            globals()[path] = MinHashLSH(\n",
    "                threshold=self.threshold,\n",
    "                num_perm=self.num_perm,\n",
    "            )\n",
    "            hashed_benchmark_ds = load_from_disk(path)\n",
    "            # Update the global variable with the MinHashLSH index.\n",
    "            with globals()[path].insertion_session() as session:\n",
    "                for record in hashed_benchmark_ds:\n",
    "                    session.insert(record[\"__id__\"], LeanMinHash(seed=MINHASH_SEED, hashvalues=record[\"__signature__\"]))\n",
    "            \n",
    "            queried = hashed_ds.map(\n",
    "                function=lambda x, y: query_content(x, y, index=globals()[path]),\n",
    "                num_proc=os.cpu_count(),\n",
    "                input_columns=[\n",
    "                    \"__id__\",\n",
    "                    \"__signature__\",\n",
    "                ],\n",
    "                remove_columns=[\"__signature__\"],\n",
    "                desc=\"Querying...\",\n",
    "                features=Features(\n",
    "                    {\n",
    "                        \"__id__\": Value(\"uint64\"),\n",
    "                        \"__neighbors__\": Sequence(Value(\"string\")),\n",
    "                    }\n",
    "                ),\n",
    "            ).filter(\n",
    "                lambda x: len(x[\"__neighbors__\"]) > 0,\n",
    "                num_proc=os.cpu_count(),\n",
    "                desc=f\"Filtering...\",\n",
    "            )\n",
    "\n",
    "            # Update the set of duplicate ids.\n",
    "            for record in tqdm(\n",
    "                queried,\n",
    "                desc=f\"Checking for false positives...\" if check_for_fp else f\"Filtering...\",\n",
    "            ):\n",
    "                if check_for_fp:\n",
    "                    neighbors = set(record[\"__neighbors__\"])\n",
    "                    curr_text = ds[record[\"__id__\"]][column]\n",
    "                    for neighbor in neighbors:\n",
    "                        reference = hashed_benchmark_ds[int(neighbor)]\n",
    "                        reference_text = reference[\"__content__\"]\n",
    "                        if jaccard_similarity(curr_text, reference_text) >= self.threshold:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                dup_ids.add(record[\"__id__\"])\n",
    "\n",
    "        # Filter out the duplicate ids.\n",
    "        final_data = ds.filter(\n",
    "            lambda idx: idx not in dup_ids,\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Filtering duplicates...\",\n",
    "        )\n",
    "\n",
    "        FINAL_DATA_SIZE = len(final_data)\n",
    "        DUP_SIZE = DATA_SIZE - FINAL_DATA_SIZE\n",
    "\n",
    "        logger.info(f\"{'Data Number':<30}: {DATA_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Number':<30}: {DUP_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Rate':<30}: {DUP_SIZE / DATA_SIZE:.2%}\")\n",
    "        logger.info(f\"{'Total Time':<30}: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "benchmark_names = [\"openai_humaneval\", \"mbpp\"]\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\n",
    "bench_cleaner = BenchmarkCleaner(benchmark_names, \"/tmp/decontamination\", threshold=0.85, num_perm=256)\n",
    "ds = bench_cleaner.clean(ds, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decontamination",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
