{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset, load_from_disk, Features, Sequence, Value\n",
    "from datasketch import LeanMinHash, MinHash, MinHashLSH\n",
    "from rich.logging import RichHandler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "datasets.logging.set_verbosity_error()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(RichHandler(rich_tracebacks=True))\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MINHASH_SEED = 42\n",
    "NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hash_content(\n",
    "    idx: int, # index of the document\n",
    "    content: str, # content of the document\n",
    "    *,\n",
    "    num_perm: int # number of permutations\n",
    "    ): # The MinHash signature and the index of the record.\n",
    "    \"\"\"\n",
    "    Hash the content of a record using MinHash. This function should be\n",
    "    used with multiprocessing and it scales well with the number of cores.\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm, seed=MINHASH_SEED)\n",
    "    m.update_batch([token.encode(\"utf-8\") for token in {t for t in NON_ALPHA.split(content) if t}])\n",
    "    return {\"__signature__\": m.hashvalues, \"__id__\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hash_content(0, \"Hello world!\", num_perm=128)\n",
    "assert result[\"__id__\"] == 0\n",
    "assert result[\"__signature__\"].shape == (128,)\n",
    "assert result[\"__signature__\"].dtype == np.dtype('uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def query_content(\n",
    "    idx: int, # index of the document\n",
    "    signature: np.ndarray, # MinHash signature of the document\n",
    "    *,\n",
    "    index: MinHashLSH # The MinHashLSH index. It is shared across all processes when using multiprocessing with fork without copy.\n",
    "    ): # The query result.\n",
    "    \"\"\"\n",
    "    Query the MinHashLSH index for the record. This function can be used with multiprocessing\n",
    "    as long as the index is shared across processes.\n",
    "    Parameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"__neighbors__\": [\n",
    "            str(dup_idx)\n",
    "            for dup_idx in index.query(\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=signature),\n",
    "            )\n",
    "        ],\n",
    "        \"__id__\": idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jaccard_similarity(\n",
    "    s1: str, # The first string to compare.\n",
    "    s2: str # The second string to compare.\n",
    "    ) -> float: # The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    Calculate the jaccard similarity between two code snippets.\n",
    "    \"\"\"\n",
    "    tokens1 = set([t for t in NON_ALPHA.split(s1) if t.strip()])\n",
    "    tokens2 = set([t for t in NON_ALPHA.split(s2) if t.strip()])\n",
    "    return len(tokens1 & tokens2) / max(1, len(tokens1 | tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\n",
    "assert jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_list_to_dict(list):\n",
    "    result = {}\n",
    "    for item in list:\n",
    "        config = item['config']\n",
    "        split = item['split']\n",
    "        if split == \"train\": continue\n",
    "        if config in result:\n",
    "            result[config].append(split)\n",
    "        else:\n",
    "            result[config] = [split]\n",
    "    \n",
    "    final_result = {}\n",
    "    for config, splits in result.items():\n",
    "        if \"all\" in config:\n",
    "            final_result = {config: splits}\n",
    "            break\n",
    "        else:\n",
    "            final_result[config] = splits\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_lists(name):\n",
    "    token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    if token is None:\n",
    "        raise ValueError(\"HF_ACCESS_TOKEN is not set\")\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    API_URL = f\"https://datasets-server.huggingface.co/splits?dataset={name}\"\n",
    "    def query():\n",
    "        response = requests.request(\"GET\", API_URL, headers=headers)\n",
    "        return response.json()\n",
    "    data = query()\n",
    "\n",
    "    return convert_list_to_dict(data[\"splits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plain_text': ['test', 'validation']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "os.environ[\"HF_ACCESS_TOKEN\"] = \"<TOKEN>\"\n",
    "ds_dict = config_lists(\"lambada\")\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_ds_config(name, ds_dict):\n",
    "    for config, splits in ds_dict.items():\n",
    "        for split in splits:\n",
    "            # print(name, config, split)\n",
    "            ds = load_dataset(name, config, split=split)\n",
    "            remove_columns = []\n",
    "            for column, val_type in ds.features.items():\n",
    "                if val_type.dtype != \"string\":\n",
    "                    remove_columns.append(column)\n",
    "            \n",
    "            ds = ds.remove_columns(remove_columns)\n",
    "            yield ds, f\"{name}_{config}_{split}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'domain': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "\n",
    "ds, name = next(process_ds_config(\"lambada\", ds_dict))\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean the benchmark dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        benchmark_names: list, # The list of benchmark names to clean.\n",
    "        output_dir: str, # The output directory to save the cleaned datasets and intermediate results.\n",
    "        threshold: float = 0.5, # The threshold to use for the MinHashLSH index.\n",
    "        num_perm: int = 128 # The number of permutations to use for the MinHashLSH index.\n",
    "        ):\n",
    "        self.bm_names = benchmark_names\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.hash_benchmark_datasets()\n",
    "    \n",
    "    def hash_benchmark_datasets(self):\n",
    "        self.benchmarks_paths = [os.path.join(self.output_dir, d) for d in os.listdir(self.output_dir) if os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        if len(self.benchmarks_paths) == 0:\n",
    "            for name in self.bm_names:\n",
    "                ds_dict = config_lists(name)\n",
    "                for benchmark_ds, config_name in process_ds_config(name, ds_dict):\n",
    "                    benchmark_ds = benchmark_ds.map(\n",
    "                            function=lambda x, idx: {\n",
    "                                **hash_content(\n",
    "                                    idx,\n",
    "                                    \" \".join(\n",
    "                                        [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                    ),\n",
    "                                    num_perm=self.num_perm,\n",
    "                                ),\n",
    "                                \"__content__\": \" \".join(\n",
    "                                    [x[col] for col in benchmark_ds.column_names if x[col] is not None]\n",
    "                                ),\n",
    "                            },\n",
    "                            num_proc=4,\n",
    "                            with_indices=True,\n",
    "                            desc=f\"Fingerprinting...\",\n",
    "                        )\n",
    "                    # Save the benchmark dataset.\n",
    "                    benchmarks_path = os.path.join(self.output_dir, config_name)\n",
    "                    benchmark_ds.save_to_disk(benchmarks_path, max_shard_size=\"1GB\")\n",
    "                    self.benchmarks_paths.append(benchmarks_path)\n",
    "        else:\n",
    "            logger.info(\"Benchmark datasets already exist. Skipping hashing.\")\n",
    "\n",
    "    def clean(\n",
    "        self,\n",
    "        ds: Dataset, # The dataset to clean.\n",
    "        column: str, # The column to clean.\n",
    "        check_for_fp: bool = True, # Whether to check for false positives.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Clean the dataset. This function does the following:\n",
    "        1. Hash the content of the provided dataset using MinHash.\n",
    "        2. Iterate over the benchmark datasets and hash their content.\n",
    "        3. Query the MinHashLSH index for each record in the provided dataset against the benchmark datasets.\n",
    "        4. Filter out the records that have a high similarity with the benchmark datasets.\n",
    "        5. Return the cleaned dataset.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        DATA_SIZE = len(ds)\n",
    "        ds = ds.map(\n",
    "            lambda _, idx: {\"__id__\": idx},\n",
    "            with_indices=True,\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Adding index...\",\n",
    "        )\n",
    "        hashed_ds = ds.map(\n",
    "            function=hash_content,\n",
    "            fn_kwargs={\"num_perm\": self.num_perm},\n",
    "            input_columns=[\"__id__\", column],\n",
    "            remove_columns=[column],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=f\"Fingerprinting...\",\n",
    "        )\n",
    "        # remove unused columns\n",
    "        hashed_ds = hashed_ds.remove_columns([c for c in hashed_ds.column_names if c not in [\"__id__\", \"__signature__\"]])\n",
    "        dup_ids = set() # The set of duplicate ids that should be filtered out.\n",
    "        # Iterate over the benchmark datasets, hash their content and query the MinHashLSH index.\n",
    "        for path in self.benchmarks_paths:\n",
    "            globals()[path] = MinHashLSH(\n",
    "                threshold=self.threshold,\n",
    "                num_perm=self.num_perm,\n",
    "            )\n",
    "            hashed_benchmark_ds = load_from_disk(path)\n",
    "            # Update the global variable with the MinHashLSH index.\n",
    "            with globals()[path].insertion_session() as session:\n",
    "                for record in hashed_benchmark_ds:\n",
    "                    session.insert(record[\"__id__\"], LeanMinHash(seed=MINHASH_SEED, hashvalues=record[\"__signature__\"]))\n",
    "            \n",
    "            queried = hashed_ds.map(\n",
    "                function=lambda x, y: query_content(x, y, index=globals()[path]),\n",
    "                num_proc=os.cpu_count(),\n",
    "                input_columns=[\n",
    "                    \"__id__\",\n",
    "                    \"__signature__\",\n",
    "                ],\n",
    "                remove_columns=[\"__signature__\"],\n",
    "                desc=\"Querying...\",\n",
    "                features=Features(\n",
    "                    {\n",
    "                        \"__id__\": Value(\"uint64\"),\n",
    "                        \"__neighbors__\": Sequence(Value(\"string\")),\n",
    "                    }\n",
    "                ),\n",
    "            ).filter(\n",
    "                lambda x: len(x[\"__neighbors__\"]) > 0,\n",
    "                num_proc=os.cpu_count(),\n",
    "                desc=f\"Filtering...\",\n",
    "            )\n",
    "\n",
    "            # Update the set of duplicate ids.\n",
    "            for record in tqdm(\n",
    "                queried,\n",
    "                desc=f\"Checking for false positives...\" if check_for_fp else f\"Filtering...\",\n",
    "            ):\n",
    "                if check_for_fp:\n",
    "                    neighbors = set(record[\"__neighbors__\"])\n",
    "                    curr_text = ds[record[\"__id__\"]][column]\n",
    "                    for neighbor in neighbors:\n",
    "                        reference = hashed_benchmark_ds[int(neighbor)]\n",
    "                        reference_text = reference[\"__content__\"]\n",
    "                        if jaccard_similarity(curr_text, reference_text) >= self.threshold:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                dup_ids.add(record[\"__id__\"])\n",
    "\n",
    "        # Filter out the duplicate ids.\n",
    "        final_data = ds.filter(\n",
    "            lambda idx: idx not in dup_ids,\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Filtering duplicates...\",\n",
    "        )\n",
    "\n",
    "        FINAL_DATA_SIZE = len(final_data)\n",
    "        DUP_SIZE = DATA_SIZE - FINAL_DATA_SIZE\n",
    "\n",
    "        logger.info(f\"{'Data Number':<30}: {DATA_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Number':<30}: {DUP_SIZE}\")\n",
    "        logger.info(f\"{'Duplicate Rate':<30}: {DUP_SIZE / DATA_SIZE:.2%}\")\n",
    "        logger.info(f\"{'Total Time':<30}: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/29/23 02:11:22] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Benchmark datasets already exist. Skipping hashing.                   <a href=\"file:///tmp/ipykernel_386164/2458772748.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2458772748.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_386164/2458772748.py#49\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/29/23 02:11:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Benchmark datasets already exist. Skipping hashing.                   \u001b]8;id=683242;file:///tmp/ipykernel_386164/2458772748.py\u001b\\\u001b[2m2458772748.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=894640;file:///tmp/ipykernel_386164/2458772748.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n",
      "Checking for false positives...: 100%|██████████| 8636/8636 [01:03<00:00, 135.39it/s]\n",
      "Checking for false positives...: 100%|██████████| 3552/3552 [00:01<00:00, 2593.27it/s]\n",
      "Checking for false positives...: 100%|██████████| 5533/5533 [00:04<00:00, 1110.90it/s]\n",
      "Checking for false positives...: 100%|██████████| 8360/8360 [01:26<00:00, 96.30it/s] \n",
      "Checking for false positives...: 100%|██████████| 4602/4602 [00:02<00:00, 2021.59it/s]\n",
      "Checking for false positives...: 100%|██████████| 7555/7555 [00:48<00:00, 156.90it/s]\n",
      "Checking for false positives...: 100%|██████████| 7005/7005 [00:16<00:00, 431.52it/s]\n",
      "Filtering duplicates... #2:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #0: 100%|██████████| 1/1 [00:00<00:00, 58.87ba/s]\n",
      "Filtering duplicates... #1: 100%|██████████| 1/1 [00:00<00:00, 63.38ba/s]\n",
      "Filtering duplicates... #2: 100%|██████████| 1/1 [00:00<00:00, 68.70ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #3: 100%|██████████| 1/1 [00:00<00:00, 60.64ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #5: 100%|██████████| 1/1 [00:00<00:00, 43.37ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #4: 100%|██████████| 1/1 [00:00<00:00, 32.48ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #6: 100%|██████████| 1/1 [00:00<00:00, 27.28ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #7: 100%|██████████| 1/1 [00:00<00:00, 29.50ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering duplicates... #10: 100%|██████████| 1/1 [00:00<00:00, 29.71ba/s]\n",
      "Filtering duplicates... #8: 100%|██████████| 1/1 [00:00<00:00, 30.98ba/s]\n",
      "Filtering duplicates... #9: 100%|██████████| 1/1 [00:00<00:00, 25.37ba/s]\n",
      "Filtering duplicates... #11: 100%|██████████| 1/1 [00:00<00:00, 32.13ba/s]\n",
      "Filtering duplicates... #12: 100%|██████████| 1/1 [00:00<00:00, 32.90ba/s]\n",
      "Filtering duplicates... #14: 100%|██████████| 1/1 [00:00<00:00, 66.32ba/s]\n",
      "Filtering duplicates... #13: 100%|██████████| 1/1 [00:00<00:00, 33.45ba/s]\n",
      "Filtering duplicates... #16: 100%|██████████| 1/1 [00:00<00:00, 40.28ba/s]\n",
      "Filtering duplicates... #19: 100%|██████████| 1/1 [00:00<00:00, 34.47ba/s]\n",
      "Filtering duplicates... #21: 100%|██████████| 1/1 [00:00<00:00, 42.24ba/s]\n",
      "Filtering duplicates... #22: 100%|██████████| 1/1 [00:00<00:00, 71.97ba/s]\n",
      "Filtering duplicates... #23: 100%|██████████| 1/1 [00:00<00:00, 43.33ba/s]\n",
      "Filtering duplicates... #18: 100%|██████████| 1/1 [00:00<00:00, 45.44ba/s]\n",
      "Filtering duplicates... #20: 100%|██████████| 1/1 [00:00<00:00, 48.22ba/s]\n",
      "Filtering duplicates... #17: 100%|██████████| 1/1 [00:00<00:00, 44.87ba/s]\n",
      "Filtering duplicates... #28: 100%|██████████| 1/1 [00:00<00:00, 48.62ba/s]\n",
      "Filtering duplicates... #25: 100%|██████████| 1/1 [00:00<00:00, 46.05ba/s]\n",
      "Filtering duplicates... #24: 100%|██████████| 1/1 [00:00<00:00, 42.78ba/s]\n",
      "Filtering duplicates... #15: 100%|██████████| 1/1 [00:00<00:00, 25.65ba/s]\n",
      "Filtering duplicates... #30: 100%|██████████| 1/1 [00:00<00:00, 38.18ba/s]\n",
      "Filtering duplicates... #27: 100%|██████████| 1/1 [00:00<00:00, 38.82ba/s]\n",
      "Filtering duplicates... #29: 100%|██████████| 1/1 [00:00<00:00, 17.22ba/s]\n",
      "Filtering duplicates... #31: 100%|██████████| 1/1 [00:00<00:00, 17.95ba/s]\n",
      "Filtering duplicates... #26: 100%|██████████| 1/1 [00:00<00:00, 20.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/29/23 02:15:08] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Data Number                   : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>                                <a href=\"file:///tmp/ipykernel_386164/2458772748.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2458772748.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_386164/2458772748.py#145\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/29/23 02:15:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Data Number                   : \u001b[1;36m10000\u001b[0m                                \u001b]8;id=106069;file:///tmp/ipykernel_386164/2458772748.py\u001b\\\u001b[2m2458772748.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=704921;file:///tmp/ipykernel_386164/2458772748.py#145\u001b\\\u001b[2m145\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Duplicate Number              : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                    <a href=\"file:///tmp/ipykernel_386164/2458772748.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2458772748.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_386164/2458772748.py#146\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Duplicate Number              : \u001b[1;36m0\u001b[0m                                    \u001b]8;id=181081;file:///tmp/ipykernel_386164/2458772748.py\u001b\\\u001b[2m2458772748.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=325354;file:///tmp/ipykernel_386164/2458772748.py#146\u001b\\\u001b[2m146\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Duplicate Rate                : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%                                <a href=\"file:///tmp/ipykernel_386164/2458772748.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2458772748.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_386164/2458772748.py#147\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Duplicate Rate                : \u001b[1;36m0.00\u001b[0m%                                \u001b]8;id=681870;file:///tmp/ipykernel_386164/2458772748.py\u001b\\\u001b[2m2458772748.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=432599;file:///tmp/ipykernel_386164/2458772748.py#147\u001b\\\u001b[2m147\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Total Time                    : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225.13</span> seconds                       <a href=\"file:///tmp/ipykernel_386164/2458772748.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2458772748.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_386164/2458772748.py#148\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Total Time                    : \u001b[1;36m225.13\u001b[0m seconds                       \u001b]8;id=864811;file:///tmp/ipykernel_386164/2458772748.py\u001b\\\u001b[2m2458772748.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=642909;file:///tmp/ipykernel_386164/2458772748.py#148\u001b\\\u001b[2m148\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "benchmark_names = [\"openai_humaneval\", \"mbpp\"]\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\n",
    "bench_cleaner = BenchmarkCleaner(benchmark_names, \"/tmp/decontamination\", threshold=0.85, num_perm=128)\n",
    "ds = bench_cleaner.clean(ds, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decontamination",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
