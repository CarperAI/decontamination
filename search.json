[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nhash_content\n\n hash_content (idx:int, content:str, num_perm:int)\n\nHash the content of a record using MinHash. This function should be used with multiprocessing and it scales well with the number of cores.\n\n\n\n\nType\nDetails\n\n\n\n\nidx\nint\nindex of the document\n\n\ncontent\nstr\ncontent of the document\n\n\nnum_perm\nint\n\n\n\n\n\nresult = hash_content(0, \"Hello world!\", num_perm=128)\nassert result[\"__id__\"] == 0\nassert result[\"__signature__\"].shape == (128,)\nassert result[\"__signature__\"].dtype == np.dtype('uint64')\n\n\nsource\n\n\nquery_content\n\n query_content (idx:int, signature:numpy.ndarray,\n                index:datasketch.lsh.MinHashLSH)\n\nQuery the MinHashLSH index for the record. This function can be used with multiprocessing as long as the index is shared across processes. Parameters.\n\n\n\n\nType\nDetails\n\n\n\n\nidx\nint\nindex of the document\n\n\nsignature\nndarray\nMinHash signature of the document\n\n\nindex\nMinHashLSH\n\n\n\n\n\nsource\n\n\njaccard_similarity\n\n jaccard_similarity (s1:str, s2:str)\n\nCalculate the jaccard similarity between two code snippets.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ns1\nstr\nThe first string to compare.\n\n\ns2\nstr\nThe second string to compare.\n\n\nReturns\nfloat\nThe Jaccard similarity between the two strings.\n\n\n\n\nassert jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\nassert jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0\n\n\nsource\n\n\nconvert_list_to_dict\n\n convert_list_to_dict (list)\n\n\nsource\n\n\nconfig_lists\n\n config_lists (name)\n\n\nos.environ[\"HF_ACCESS_TOKEN\"] = \"<TOKEN>\"\nds_dict = config_lists(\"lambada\")\nds_dict\n\n{'plain_text': ['test', 'validation']}\n\n\n\nsource\n\n\nprocess_ds_config\n\n process_ds_config (name, ds_dict, output_dir)\n\n\nds, name = next(process_ds_config(\"lambada\", ds_dict, \"/tmp\"))\nds.features\n\nDownloading builder script: 100%|██████████| 4.92k/4.92k [00:00<00:00, 2.58MB/s]\nDownloading readme: 100%|██████████| 7.06k/7.06k [00:00<00:00, 6.25MB/s]\n\n\nDownloading and preparing dataset lambada/plain_text to /home/nathan/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373...\n\n\n                                                                                         \n\n\nDataset lambada downloaded and prepared to /home/nathan/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373. Subsequent calls will reuse this data.\n\n\n\n\n\n{'text': Value(dtype='string', id=None),\n 'domain': Value(dtype='string', id=None)}\n\n\n\nsource\n\n\nparallelized_function\n\n parallelized_function (queried, check_for_fp, ds, column, benchmarks,\n                        threshold, num_workers)\n\n\nsource\n\n\nprocess_record\n\n process_record (record, check_for_fp, ds, column, benchmarks, threshold)\n\n\nsource\n\n\nBenchmarkCleaner\n\n BenchmarkCleaner (benchmark_names:list, output_dir:str,\n                   threshold:float=0.5, num_perm:int=128,\n                   num_workers:int=1)\n\nA class to clean the benchmark dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbenchmark_names\nlist\n\nThe list of benchmark names to clean.\n\n\noutput_dir\nstr\n\nThe output directory to save the cleaned datasets and intermediate results.\n\n\nthreshold\nfloat\n0.5\nThe threshold to use for the MinHashLSH index.\n\n\nnum_perm\nint\n128\nThe number of permutations to use for the MinHashLSH index.\n\n\nnum_workers\nint\n1\nThe number of workers to use for the MinHashLSH index.\n\n\n\n\nbenchmark_names = [\"openai_humaneval\", \"mbpp\"]\nds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\nbench_cleaner = BenchmarkCleaner(benchmark_names, \"/tmp/decontamination\", threshold=0.85, num_perm=256)\nds = bench_cleaner.clean(ds, \"content\")\n\nDownloading readme: 100%|██████████| 6.40k/6.40k [00:00<00:00, 5.31MB/s]\nDownloading readme: 100%|██████████| 8.60k/8.60k [00:00<00:00, 7.81MB/s]                    \n                                                                                             \n\n\nDownloading and preparing dataset mbpp/sanitized to /home/nathan/.cache/huggingface/datasets/mbpp/sanitized/1.0.2/4458a31cd4305553c8e88e3f0bfb94fc74fe1a9faeeb8c32ed166939735eaeff...\n\n\nDownloading data: 255kB [00:00, 11.7MB/s]                    \n                                                                          \n\n\nDataset mbpp downloaded and prepared to /home/nathan/.cache/huggingface/datasets/mbpp/sanitized/1.0.2/4458a31cd4305553c8e88e3f0bfb94fc74fe1a9faeeb8c32ed166939735eaeff. Subsequent calls will reuse this data.\n\n\nLoading benchmark datasets...: 100%|██████████| 7/7 [00:00<00:00, 1006.31it/s]               \n\n\n[04/21/23 17:26:55] INFO     MinHashLSH index does not exist. Creating...                         2325999894.py:100\n\n\n\nInserting benchmarks into MinHashLSH index...: 100%|██████████| 1071/1071 [00:00<00:00, 7141.44it/s]\n\n\n                    INFO     MinHashLSH index created and saved to disk.                          2325999894.py:111\n\n\n\n                    INFO     Querying MinHashLSH index...                                         2325999894.py:113\n\n\n\n                                                                          \n\n\n[04/21/23 17:26:56] INFO     Data Number                   : 10000                                2325999894.py:152\n\n\n\n                    INFO     Duplicate Number              : 0                                    2325999894.py:153\n\n\n\n                    INFO     Duplicate Rate                : 0.00%                                2325999894.py:154\n\n\n\n                    INFO     Total Time                    : 20.31 seconds                        2325999894.py:155"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "decontamination",
    "section": "",
    "text": "This repository is heavily inspired by the BigCode repository and is mostly a refactoring of their code. Specifically, the main person who worked on this repository is Chenghao Mou (Awesome work!)."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "decontamination",
    "section": "Install",
    "text": "Install\npip install decontamination"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "decontamination",
    "section": "How to use",
    "text": "How to use\nFirst you need to specify which benchmarks you want to clean your data of. You can do this by creating dictionary with the benchmark name in huggingface’s datasets repository as the key and the name of the column containing the benchmark data as the value. For example, if you want to clean your data of the HumanEval and LAMBADA benchmarks, you would do the following:\n\n!export HF_ACCESS_TOKEN=<TOKEN>\n\n\nfrom datasets import load_dataset\nfrom decontamination.core import BenchmarkCleaner\n\n# load your dataset\ndataset = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\n\nbenchmarks = [\"openai_humaneval\", \"lambada\"]\ncleaner = BenchmarkCleaner(benchmarks, \"/tmp/benchmarks\", threshold=0.1, num_perm=128)\n\n# clean the dataset\ncleaned_dataset = cleaner.clean(dataset, column=\"content\", check_for_fp=True)\n\n[01/24/23 00:27:37] INFO     Benchmark datasets already exist. Skipping hashing.                        core.py:181\n\n\n\n/home/nathan/miniconda3/envs/decontamination/lib/python3.10/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\nYou can remove this warning by passing 'storage_options=fs.storage_options' instead.\n  warnings.warn(\nChecking for false positives...: 100%|██████████| 8636/8636 [00:33<00:00, 261.25it/s]\nChecking for false positives...: 100%|██████████| 8805/8805 [06:58<00:00, 21.06it/s]\nChecking for false positives...: 100%|██████████| 8722/8722 [06:39<00:00, 21.82it/s]\nFiltering duplicates... #0: 100%|██████████| 1/1 [00:00<00:00, 140.36ba/s]\n\nFiltering duplicates... #1: 100%|██████████| 1/1 [00:00<00:00, 123.28ba/s]\nFiltering duplicates... #2: 100%|██████████| 1/1 [00:00<00:00, 169.47ba/s]\n\n\n\n\n\nFiltering duplicates... #3: 100%|██████████| 1/1 [00:00<00:00, 141.77ba/s]\n\n\n\n\nFiltering duplicates... #4: 100%|██████████| 1/1 [00:00<00:00, 142.31ba/s]\n\n\n\n\n\nFiltering duplicates... #5: 100%|██████████| 1/1 [00:00<00:00, 139.13ba/s]\n\n\n\n\n\n\nFiltering duplicates... #6: 100%|██████████| 1/1 [00:00<00:00, 156.00ba/s]\n\n\n\n\n\n\n\nFiltering duplicates... #7: 100%|██████████| 1/1 [00:00<00:00, 139.18ba/s]\n\n\n\n\n\n\n\n\nFiltering duplicates... #8: 100%|██████████| 1/1 [00:00<00:00, 162.53ba/s]\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #9: 100%|██████████| 1/1 [00:00<00:00, 140.68ba/s]\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #10: 100%|██████████| 1/1 [00:00<00:00, 138.69ba/s]\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #11: 100%|██████████| 1/1 [00:00<00:00, 145.31ba/s]\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #12: 100%|██████████| 1/1 [00:00<00:00, 144.74ba/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #13: 100%|██████████| 1/1 [00:00<00:00, 157.68ba/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #14: 100%|██████████| 1/1 [00:00<00:00, 95.45ba/s]\nFiltering duplicates... #15: 100%|██████████| 1/1 [00:00<00:00, 135.26ba/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #16: 100%|██████████| 1/1 [00:00<00:00, 136.07ba/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering duplicates... #17: 100%|██████████| 1/1 [00:00<00:00, 107.33ba/s]\nFiltering duplicates... #18: 100%|██████████| 1/1 [00:00<00:00, 141.83ba/s]\nFiltering duplicates... #19: 100%|██████████| 1/1 [00:00<00:00, 139.11ba/s]\nFiltering duplicates... #20: 100%|██████████| 1/1 [00:00<00:00, 137.10ba/s]\nFiltering duplicates... #21: 100%|██████████| 1/1 [00:00<00:00, 146.80ba/s]\nFiltering duplicates... #22: 100%|██████████| 1/1 [00:00<00:00, 147.25ba/s]\nFiltering duplicates... #23: 100%|██████████| 1/1 [00:00<00:00, 149.84ba/s]\nFiltering duplicates... #24: 100%|██████████| 1/1 [00:00<00:00, 132.19ba/s]\nFiltering duplicates... #25: 100%|██████████| 1/1 [00:00<00:00, 24.02ba/s]\nFiltering duplicates... #30: 100%|██████████| 1/1 [00:00<00:00, 119.37ba/s]\nFiltering duplicates... #29: 100%|██████████| 1/1 [00:00<00:00, 98.58ba/s]\nFiltering duplicates... #28: 100%|██████████| 1/1 [00:00<00:00, 85.76ba/s]\nFiltering duplicates... #26: 100%|██████████| 1/1 [00:00<00:00, 76.09ba/s]\nFiltering duplicates... #31: 100%|██████████| 1/1 [00:00<00:00, 69.66ba/s]\nFiltering duplicates... #27: 100%|██████████| 1/1 [00:00<00:00, 62.54ba/s]\n\n\n[01/24/23 00:41:50] INFO     Data Number                   : 10000                                      core.py:277\n\n\n\n                    INFO     Duplicate Number              : 3932                                       core.py:278\n\n\n\n                    INFO     Duplicate Rate                : 39.32%                                     core.py:279\n\n\n\n                    INFO     Total Time                    : 853.66 seconds                             core.py:280"
  }
]